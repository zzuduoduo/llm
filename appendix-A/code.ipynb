{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# menu\n",
    "- PyTorch介绍\n",
    "- 理解Tensors\n",
    "- 计算图自动微分\n",
    "- 多层神经网络\n",
    "- 典型训练模式\n",
    "- 保存和加载模型\n",
    "- 如何使用GPU(英伟达和苹果GPU)加速训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI相关概念说明\n",
    "![AI概念](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image003.png) \n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image005.png)\n",
    "大模型在工作和任务上的用途"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.PyTorch的发展历史\n",
    "PyTorch是一个由Facebook人工智能研究院（FAIR）开发的深度学习框架，自2017年1月首次发布。基于Torch结合python的方式进行改造，以打造一个更加易用的机器学习框架。\n",
    "- 2017年1月：PyTorch 0.1.0发布，这是PyTorch的第一个版本，包括了张量计算和基础的神经网络层 。2017年8月：PyTorch 0.2.0发布，引入了更多张量计算功能，如广播机制和高效的张量操作，同时添加了新的数据加载和预处理工具 。\n",
    "- 2018年1月：PyTorch 1.0.0发布，标志着框架进入新阶段，引入了动态计算图、高效的内存管理、多GPU支持和更强大的可视化工具，同时支持Python 3.5及以上版本 。\n",
    "- 2018年9月：PyTorch 1.2.0发布，增加了新的张量操作和改进的模型训练过程，引入了高效的序列建模工具，如LSTM和GRU，并加强了分布式训练的支持 。\n",
    "- 2019年3月：PyTorch 1.4.0引入了混合精度训练和动态神经网络模块，允许使用低精度数据加速训练并减少内存使用 。\n",
    "- 2019年9月：PyTorch 1.6.0添加了新的张量运算和自动混合精度训练，提高了训练速度并减少了内存使用，同时改进了模型序列化和分布式训练性能 。\n",
    "- 2020年3月：PyTorch 1.8.0引入了更强大的数据加载和处理工具，包括DataLoader的并行加载和分布式数据加载，添加了新的激活函数和层 。\n",
    "- 2021年9月：PyTorch 2.0.0发布，引入了对更高效训练、推理和调试的支持，优化了可扩展性和可维护性，提供了对自动微分和调试工具的改进，并支持更多硬件平台和操作系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch在学术界的使用情况\n",
    "![](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch三大核心组件\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装PyTorch\n",
    "https://pytorch.org/get-started/locally/ 安装建议参考网址\n",
    "\n",
    "pip3 install torch torchvision torchaudio 其中torchvision torchaudio属于可选\n",
    "![](image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查torch的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有cuda，可以检查安装的pytorch是否支持cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是m系列的macbook，可以用下面的方式来检查pytorch是否支持apple silicon chip的GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前最新版的pytorch已经实验性支持AMD的GPU，不过只能在linux上使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有GPU，也没有关系，这次学习主要是学习大模型的基础，并不会强依赖GPU，但后面训练和调优如果有GPU加持，效率会提升很多。如果想体验GPU的话，可以通过科学上网使用google的colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二.张量tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tensor(张量)是现代深度学习中最重要一个概念，TensorFlow直接使用tensor来开头命名的。\n",
    "\n",
    "##### 大学线性代数里都学过标量(scalar),向量(vector)，矩阵(matric),其中标量是0维，向量是1维，矩阵是2维，使用秩(rank)来标识维度\n",
    "\n",
    "##### tensor是把所有维度的数据统一了起来，同时能够更好的支持计算，在深度学习中最重要的是支持自动微分和支持GPU\n",
    "\n",
    "##### 在编程上可以把tensor看作一个对象，除了存储多维数据外，里面封装了大量的方法。\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image011.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用torch.tensor来创建tensor类\n",
    "tensor0d = torch.tensor(1)\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "tensor2d = torch.tensor([[1,2],[3,4]])\n",
    "tensor3d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# tensor data types\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# float类型的tensor\n",
    "floatvec = torch.tensor([1.0,2.0,3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pytorch默认使用32位的float，原因是节省内存和计算量，同时GPU架构对32为做了特殊优化，能够加快训练和推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 不同精度可以通过.to 方法进行转换\n",
    "floatvec1 = tensor1d.to(torch.float32)\n",
    "print(floatvec1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多关于tensor数据类型的介绍，建议参考pytorch的官网：https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍对tensor的常用操作方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "#1 torch.tensor()创建新的tensor\n",
    "tensor2d = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# .shape属性获取tensor的维度或者秩（shape）\n",
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# tensor2d 是一个2行，3列的矩阵，如果想把它转换成3行2列的矩阵，可以使用reshpae()\n",
    "print(tensor2d.reshape(3,2))\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 比reshape更常用的方法是.view()方法，reshape()方法是torch原有的\n",
    "# view()是NumPy的方式，也是目前深度学习框架最为常用的方式\n",
    "print(tensor2d.view(3,2))\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# .T 转置矩阵,行列内容互换\n",
    "print(tensor2d.T)\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# .matmul()方法张量相乘\n",
    "print(tensor2d.matmul(tensor2d.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用@操作符进行张量相乘\n",
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computation graph(计算图)\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200527151747/e19.png)\n",
    "\n",
    "计算图：单向无环图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归模型\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image013.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2])\n",
    "b = torch.tensor([0.0])\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动微分\n",
    "求极值的方法：\n",
    "\n",
    "![](https://img2020.cnblogs.com/blog/1522661/202012/1522661-20201217225019636-508712522.png)\n",
    "\n",
    "沿着导数反方向可以得到极小值，\n",
    "\n",
    "链式法则\n",
    "\n",
    "反向传播算法(backpropagation)\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image015.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度通过autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2],requires_grad=True)\n",
    "b = torch.tensor([0.0],requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "#retain_graph表示每次反向传播（计算微分）后是否保留计算图，这里由于需要对两个参数进行进行计算梯度\n",
    "#因此第一次计算时候需要保留计算图，第二次就不用保留计算图\n",
    "grad_L_w1 = grad(loss,w1,retain_graph=True)\n",
    "grad_L_b = grad(loss,b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# 上面是手工计算梯度，pytorch提供了更方便的方法，自动计算梯度，并把梯度存储在张量的grad属性上\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 如果你忘记了微积分相关领域的知识，对于以上所讲内容不是很理解，也不用担心，你只需记住pytorch提供了.backward方法来自动计算梯度，一个方法帮你搞定最难的地方，这就是pytorch框架厉害之处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多层深层网络实现\n",
    "pytorch作为深度学习框架，可以很方便的实现深度神经网络\n",
    "\n",
    "这里咱们通过pytorch实现一个简单的深度神经网络例子\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image017.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层隐藏层的多层神经网络\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 第一隐藏层\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 第二隐藏层\n",
    "            torch.nn.Linear(30,20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 输出层\n",
    "            torch.nn.Linear(20,num_outputs),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个模型\n",
    "model = NeuralNetwork(50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential方法比较方便的将各个神经网络层进行有序组装起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "# 查明所有的参数数量\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\",num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1388,  0.0159,  0.1215,  ...,  0.1032,  0.0296,  0.0102],\n",
      "        [ 0.0229,  0.0260, -0.0458,  ..., -0.0358,  0.0362,  0.0497],\n",
      "        [-0.0896,  0.0113,  0.1370,  ...,  0.1037,  0.1230, -0.0929],\n",
      "        ...,\n",
      "        [-0.1362, -0.0713, -0.0010,  ...,  0.1176,  0.1054, -0.1012],\n",
      "        [ 0.1226,  0.0937, -0.1409,  ...,  0.1321, -0.0613,  0.0086],\n",
      "        [-0.0045, -0.0604,  0.0535,  ...,  0.0697,  0.0373,  0.0923]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)\n",
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0181,  0.1404,  0.0374,  0.1102,  0.0045,  0.0788,  0.1013,  0.0211,\n",
      "         0.1191, -0.1204, -0.0152, -0.0222, -0.0056,  0.0466, -0.0365, -0.0321,\n",
      "         0.0927, -0.1029, -0.0093,  0.1047,  0.1279, -0.1176,  0.0445,  0.0583,\n",
      "         0.0263,  0.0459, -0.0549,  0.0258,  0.0305,  0.0463],\n",
      "       requires_grad=True)\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# 获取偏参数\n",
    "print(model.layers[0].bias)\n",
    "print(model.layers[0].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以观察下这些张量的值都是比较小的随机值，大家显示的结果和我这边肯定不一样的。原因是在深度学习中，模型参数特意设定为随机值，以此来保证学习的效果，太大容易梯度爆炸，太小容易梯度消失，相同的值导致模型没法学习\n",
    "\n",
    "那如何保证每次模型参数初始化都一样，这样可以验证每次的结果呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50,3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们接下来看看如何产生输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "X = torch.rand((1,50))\n",
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单解释下grad_fn=<AddmmBackward0>,AddmmBackward0 分为Add(加)和mm(matrix multiplication)两个操作，这个是告诉pytorch,是如何产生结果的，然后在计算微分时候，就可以调用对应的求导方法了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是用已经训练好的模型，那么就不用计算微分，也就用不到计算梯度，这样可以节省大量的计算耗时和内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，结果输出层没有激活函数，因为pytorch会把激活函数和损失函数集成在一个方法中，这样计算更高效，所以如果想计算出输出结果的概率值，需要主动调用softmax函数才行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X),dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要注意下dim=1,model(X)一个(1,3)的矩阵，dim表示是在哪一个维度操作，dim=1表示是在列的维度操作，就是行不变，变化的是列上值。\n",
    "softmax的公式是\n",
    "![](image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何建立有效的数据加载器\n",
    "###### 在训练过程中，如何有效加载是一个很重要的事情，这里就简单讨论下如何建立自己的数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image019.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建比较小的数据集\n",
    "X_train = torch.tensor([\n",
    "    [-1.2,3.1],\n",
    "    [-0.9,2.9],\n",
    "    [-0.5,2.6],\n",
    "    [2.3,-1.1],\n",
    "    [2.7,-1.5]\n",
    "])\n",
    "y_train = torch.tensor([0,0,0,1,1])\n",
    "X_test = torch.tensor([\n",
    "    [-0.8,2.1],\n",
    "    [2.6,-1.6]\n",
    "])\n",
    "y_test = torch.tensor([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们创建一个基于上面数据集的Dataset类\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self,X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    def __getitem__(self,index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x,one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ToyDataset(X_train,y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化数据加载器\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "#遍历dataloader\n",
    "for idx, (x,y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\",x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "#遍历dataloader,每次迭代的结果是不一样，这样是刻意设计的，目的是防止“重复更新循环”，\n",
    "# 提高模型的泛化能力\n",
    "for idx, (x,y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\",x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#丢弃最后一个data case,防止训练过程中出现无法收敛的情况\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-0.9000,  2.9000],\n",
      "        [ 2.3000, -1.1000]]) tensor([0, 1])\n",
      "Batch 2: tensor([[ 2.7000, -1.5000],\n",
      "        [-0.5000,  2.6000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x,y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\",x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并行数据加载num_workers\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image021.png)\n",
    "\n",
    "num_workers=0表示没有并行加载，1或者大于1表示会在后台进行加载数据，并把数据放在队列里，但不建议在小数据量和jupyter notebook里使用。作者建议设置4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经典的训练过程\n",
    "典型的训练循环（Training Loop）是机器学习或深度学习中用于模型训练的代码结构。这个循环负责迭代地处理训练数据，执行前向传播和反向传播，以及更新模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001/003 | Batch 001/002 | Train Loss:0.75\n",
      "Epoch:001/003 | Batch 002/002 | Train Loss:0.65\n",
      "Epoch:002/003 | Batch 001/002 | Train Loss:0.44\n",
      "Epoch:002/003 | Batch 002/002 | Train Loss:0.13\n",
      "Epoch:003/003 | Batch 001/002 | Train Loss:0.03\n",
      "Epoch:003/003 | Batch 002/002 | Train Loss:0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2,num_outputs=2)\n",
    "\n",
    "#选择优化器模式，主要用于更新模型参数:stochastic gradient descent (SGD) 随机梯度下降算法\n",
    "#lr表示学习速度,learn rate\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #进入训练模式\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (features, lables) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        #交叉墒损失函数\n",
    "        loss = F.cross_entropy(logits, lables)\n",
    "\n",
    "        #梯度归零，防止梯度累加\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch:{epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train Loss:{loss:.2f}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型更新公式\n",
    "\n",
    "![](image4.png)\n",
    "\n",
    "这里学习速率就是所谓的超参，意思需要时人工去调整，太大不容易收敛，太小收敛速度太慢，相当于一个人下坡时候，如果加速度太快，很容易到达谷底时候继续爬上去，如果速度太慢，则需要很长时间才能达到谷底\n",
    "\n",
    "之前调侃深度学习工程师是调参大师或者炼丹师，就是这个说需要调整这些超参的。训练轮数也是超参。凡是需要人工调整的参数都是超参\n",
    "\n",
    "那如何来调整这些超参呢，一般做法是从训练数据集里取出一部分数据当作验证集，这些验证集可以多次使用，直到找到合适的超参，而测试数据集只能使用一次就是验证模型结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()和model.eval()这两个主要是用于告诉pytorch目前模型处于什么阶段，在不同的阶段，模型结构是不一样的，比如在训练阶段，可能会加入dropout()方法，这个主要是随机将一些神经元进行隐藏，不参与训练过程，主要是提高模型的泛化能力的，但在评估阶段和使用阶段，这个dropout就不再需要。这里写的demo暂时不涉及到，但为了训练框架的一致性，也调用了这两个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "# 验证下刚才训练好的模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n"
     ]
    }
   ],
   "source": [
    "#获得输出结果对应的概率\n",
    "#输出结果非科学模式，更易读\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs,dim=1)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一行数据表示99.91%概率是class 0.09%属于class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那如何直接获取归属到哪个类别呢，就是取每一行值最大的index值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(probas,dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#我们也可以不用计算输出结果的概率，而是直接获取归属类别\n",
    "predictions = torch.argmax(outputs,dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对比模型的结果和实际结果\n",
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以计算预测正确的次数\n",
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型准确度的通用方法\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features,labels) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits,dim=1)\n",
    "        compare = predictions == labels\n",
    "        correct += torch.sum(compare).item()\n",
    "        total_examples += len(compare)\n",
    "    return correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model,train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model,test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "torch.save(model.state_dict(),\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "model1 = NeuralNetwork(2,2)\n",
    "model1.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用GPU加速训练和推理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#是否支持cuda\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "#实例化两个张量，默认是在CPU上计算\n",
    "tensor_1 = torch.tensor([1.,2.,3.])\n",
    "tensor_2 = torch.tensor([4.,5.,6.])\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取GPU的device\n",
    "device_name = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device_name=\"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name=\"mps\"\n",
    "else:\n",
    "    device_name=\"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = tensor_1.to(device)\n",
    "tensor_2 = tensor_2.to(device)\n",
    "print(tensor_1+tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_1 = tensor_1.to(device)\n",
    "#tensor_2 = tensor_2.to(\"cpu\")\n",
    "#print(tensor_1+tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001/003 | Batch 001/002 | Train Loss:0.75\n",
      "Epoch:001/003 | Batch 002/002 | Train Loss:0.65\n",
      "Epoch:002/003 | Batch 001/002 | Train Loss:0.44\n",
      "Epoch:002/003 | Batch 002/002 | Train Loss:0.13\n",
      "Epoch:003/003 | Batch 001/002 | Train Loss:0.03\n",
      "Epoch:003/003 | Batch 002/002 | Train Loss:0.00\n"
     ]
    }
   ],
   "source": [
    "#模型计算在GPU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2,num_outputs=2)\n",
    "model = model.to(device)\n",
    "\n",
    "#选择优化器模式，主要用于更新模型参数:stochastic gradient descent (SGD) 随机梯度下降算法\n",
    "#lr表示学习速度,learn rate\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #进入训练模式\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (features, lables) in enumerate(train_loader):\n",
    "        features, lables = features.to(device), lables.to(device)\n",
    "        logits = model(features)\n",
    "\n",
    "        #交叉墒损失函数\n",
    "        loss = F.cross_entropy(logits, lables)\n",
    "\n",
    "        #梯度归零，防止梯度累加\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch:{epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx+1:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train Loss:{loss:.2f}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
