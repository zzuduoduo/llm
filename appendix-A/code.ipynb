{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# menu\n",
    "- PyTorch介绍\n",
    "- 理解Tensors\n",
    "- 计算图自动微分\n",
    "- 多层神经网络\n",
    "- 典型训练模式\n",
    "- 保存和加载模型\n",
    "- 如何使用GPU(英伟达和苹果GPU)加速训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI相关概念说明\n",
    "![AI概念](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image003.png) \n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image005.png)\n",
    "大模型在工作和任务上的用途"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.PyTorch的发展历史\n",
    "PyTorch是一个由Facebook人工智能研究院（FAIR）开发的深度学习框架，自2017年1月首次发布。基于Torch结合python的方式进行改造，以打造一个更加易用的机器学习框架。\n",
    "- 2017年1月：PyTorch 0.1.0发布，这是PyTorch的第一个版本，包括了张量计算和基础的神经网络层 。2017年8月：PyTorch 0.2.0发布，引入了更多张量计算功能，如广播机制和高效的张量操作，同时添加了新的数据加载和预处理工具 。\n",
    "- 2018年1月：PyTorch 1.0.0发布，标志着框架进入新阶段，引入了动态计算图、高效的内存管理、多GPU支持和更强大的可视化工具，同时支持Python 3.5及以上版本 。\n",
    "- 2018年9月：PyTorch 1.2.0发布，增加了新的张量操作和改进的模型训练过程，引入了高效的序列建模工具，如LSTM和GRU，并加强了分布式训练的支持 。\n",
    "- 2019年3月：PyTorch 1.4.0引入了混合精度训练和动态神经网络模块，允许使用低精度数据加速训练并减少内存使用 。\n",
    "- 2019年9月：PyTorch 1.6.0添加了新的张量运算和自动混合精度训练，提高了训练速度并减少了内存使用，同时改进了模型序列化和分布式训练性能 。\n",
    "- 2020年3月：PyTorch 1.8.0引入了更强大的数据加载和处理工具，包括DataLoader的并行加载和分布式数据加载，添加了新的激活函数和层 。\n",
    "- 2021年9月：PyTorch 2.0.0发布，引入了对更高效训练、推理和调试的支持，优化了可扩展性和可维护性，提供了对自动微分和调试工具的改进，并支持更多硬件平台和操作系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch在学术界的使用情况\n",
    "![](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch三大核心组件\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装PyTorch\n",
    "https://pytorch.org/get-started/locally/ 安装建议参考网址\n",
    "\n",
    "pip3 install torch torchvision torchaudio 其中torchvision torchaudio属于可选\n",
    "![](image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查torch的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有cuda，可以检查安装的pytorch是否支持cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是m系列的macbook，可以用下面的方式来检查pytorch是否支持apple silicon chip的GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前最新版的pytorch已经实验性支持AMD的GPU，不过只能在linux上使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有GPU，也没有关系，这次学习主要是学习大模型的基础，并不会强依赖GPU，但后面训练和调优如果有GPU加持，效率会提升很多。如果想体验GPU的话，可以通过科学上网使用google的colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二.张量tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tensor(张量)是现代深度学习中最重要一个概念，TensorFlow直接使用tensor来开头命名的。\n",
    "\n",
    "##### 大学线性代数里都学过标量(scalar),向量(vector)，矩阵(matric),其中标量是0维，向量是1维，矩阵是2维，使用秩(rank)来标识维度\n",
    "\n",
    "##### tensor是把所有维度的数据统一了起来，同时能够更好的支持计算，在深度学习中最重要的是支持自动微分和支持GPU\n",
    "\n",
    "##### 在编程上可以把tensor看作一个对象，除了存储多维数据外，里面封装了大量的方法。\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image011.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用torch.tensor来创建tensor类\n",
    "tensor0d = torch.tensor(1)\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "tensor2d = torch.tensor([[1,2],[3,4]])\n",
    "tensor3d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# tensor data types\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# float类型的tensor\n",
    "floatvec = torch.tensor([1.0,2.0,3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pytorch默认使用32位的float，原因是节省内存和计算量，同时GPU架构对32为做了特殊优化，能够加快训练和推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 不同精度可以通过.to 方法进行转换\n",
    "floatvec1 = tensor1d.to(torch.float32)\n",
    "print(floatvec1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多关于tensor数据类型的介绍，建议参考pytorch的官网：https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍对tensor的常用操作方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "#1 torch.tensor()创建新的tensor\n",
    "tensor2d = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# .shape属性获取tensor的维度或者秩（shape）\n",
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# tensor2d 是一个2行，3列的矩阵，如果想把它转换成3行2列的矩阵，可以使用reshpae()\n",
    "print(tensor2d.reshape(3,2))\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 比reshape更常用的方法是.view()方法，reshape()方法是torch原有的\n",
    "# view()是NumPy的方式，也是目前深度学习框架最为常用的方式\n",
    "print(tensor2d.view(3,2))\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# .T 转置矩阵,行列内容互换\n",
    "print(tensor2d.T)\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# .matmul()方法张量相乘\n",
    "print(tensor2d.matmul(tensor2d.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用@操作符进行张量相乘\n",
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computation graph(计算图)\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200527151747/e19.png)\n",
    "\n",
    "计算图：单向无环图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归模型\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image013.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2])\n",
    "b = torch.tensor([0.0])\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动微分\n",
    "求极值的方法：\n",
    "\n",
    "![](https://img2020.cnblogs.com/blog/1522661/202012/1522661-20201217225019636-508712522.png)\n",
    "\n",
    "沿着导数反方向可以得到极小值，\n",
    "\n",
    "链式法则\n",
    "\n",
    "反向传播算法(backpropagation)\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image015.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度通过autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2],requires_grad=True)\n",
    "b = torch.tensor([0.0],requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss,w1,retain_graph=True)\n",
    "grad_L_b = grad(loss,b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# 上面是手工计算梯度，pytorch提供了更方便的方法，自动计算梯度，并把梯度存储在张量的grad属性上\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 如果你忘记了微积分相关领域的知识，对于以上所讲内容不是很理解，也不用担心，你只需记住pytorch提供了.backward方法来自动计算梯度，一个方法帮你搞定最难的地方，这就是pytorch框架厉害之处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多层深层网络实现\n",
    "pytorch作为深度学习框架，可以很方便的实现深度神经网络\n",
    "\n",
    "这里咱们通过pytorch实现一个简单的深度神经网络例子\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/A__image017.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层隐藏层的多层神经网络\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 第一隐藏层\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 第二隐藏层\n",
    "            torch.nn.Linear(30,20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 输出层\n",
    "            torch.nn.Linear(20,num_outputs),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个模型\n",
    "model = NeuralNetwork(50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential方法比较方便的将各个神经网络层进行有序组装起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "# 查明所有的参数数量\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\",num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0147,  0.0572, -0.0849,  ...,  0.0521,  0.0184,  0.1013],\n",
      "        [-0.1367,  0.1105, -0.0770,  ..., -0.1016, -0.0464, -0.0430],\n",
      "        [ 0.0800,  0.1356, -0.0068,  ..., -0.1081,  0.0067, -0.1106],\n",
      "        ...,\n",
      "        [ 0.0709,  0.0484, -0.0557,  ..., -0.0657, -0.0079, -0.0219],\n",
      "        [ 0.1202,  0.0278,  0.0224,  ...,  0.0912,  0.0269, -0.1011],\n",
      "        [-0.1393, -0.0993,  0.1321,  ..., -0.1304,  0.1297,  0.0178]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)\n",
    "print(model.layers[0].weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
