{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次分享主要内容是大模型最核心的地方：自注意力机制。\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image003.png)\n",
    "\n",
    "接下来会分四部分，第一部分是自注意力机制简化版，第二部分是自注意力机制，第三部分是因果注意力(causal attention)，第四部分是多头注意力(multi-head attention)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.The problem with modeling long sequences\n",
    "让咱们看下在没有自注意力机制之前如何解决长序列问题的，这里咱们拿语言翻译举例。\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图是德文翻译成英文的例子，发现单词不能逐词翻译，不然就会乱套，因为每种语言的语法不一样的，这需要模型能够看到上下文，才能做出正确的翻译。</br>\n",
    "\n",
    "那么自然想到的办法是使用encoder-decoder架构，先使用encoder看到所有输入，理解上下文，然后使用decoder逐词输出。</br>\n",
    "\n",
    "在transformer架构出来之前，最流行的encoder-decoder架构是RNN，比如LSTM和GRU。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image007.png)\n",
    "\n",
    "如上图所示，所有的输入先经过RNN encoder，得到一个隐状态，在encoder里，输出是下一步的输入，所以隐状态是包含了上下文的。然后隐状态作为decoder的输入。\n",
    "\n",
    "可以简单把hidden state理解为包含了上下文的embedding vector。\n",
    "\n",
    "RNN的缺点是，decoder无法获取之前的隐状态信息，所以需要把所有的隐状态都保存下来，这样会占用大量内存。同时长距离文本处理起来比较困难。\n",
    "\n",
    "2014年，研究人员提出了一个Bahdanau Attention的机制，通过修改encoder-decoder架构，以便decoder能够有选择的获得输入序列的不同部分。\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image009.png)\n",
    "\n",
    "如上图，在decoder阶段，针对特定输出，可以获得输入序列的不同部分，并通过attention weight来衡量输入序列不同部分对当前输出的重要性。\n",
    "\n",
    "Bahdanau Attention机制的缺点是，无法并行处理，在encoder阶段，所有的输入必须一个一个处理，在decoder阶段，当前时刻的隐状态依赖于前一个时刻的隐状态，所以也无法并行处理。\n",
    "\n",
    "三年后2017年，受到了Bahdanau Attention的启发，提出了transformer架构，使用自注意力机制来解决长距离依赖问题。\n",
    "\n",
    "Self-attention（自注意力）是一种让输入序列中的每个位置都能够在计算序列表示时关注同一序列中所有位置的机制。这种机制使得模型能够捕捉序列内部的长距离依赖关系，并且理解不同位置之间的复杂关系。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Capturing data dependencies with self-attention"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
